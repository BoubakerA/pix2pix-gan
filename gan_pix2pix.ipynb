{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to image translation: pix2pix gan\n",
    "Pix2pix gans are a sort of gans that are able to learn the mapping  between pairs of images. For example it can learn to transform black and white images into colorful images, turn google map photos into aerial images and also turn drawings into colorful images. \n",
    "This notebook is an attempt to train a pix2pix gan that learns the mapping between the drawing of a shoe and the actual RGB image of the shoe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dataset](https://www.kaggle.com/datasets/balraj98/edges2shoes-dataset) consists of shoe RGB images and their corresponding edges images(drawings). It is available on Kaggle.\n",
    "Below is an sample from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains overall about 50K pairs of images, but we will only take the first 1000 images for training and the following 100 for testing. This in order to reduce the computational time of training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/basaadi/projects/gan_trans/data/train\"\n",
    "sample_paths = glob.glob(train_dir+\"/*.jpg\")\n",
    "training_sample_paths = sample_paths[:1000]\n",
    "test_sample_paths = sample_paths[1000:1100]\n",
    "print(f\"len sample paths : {len(training_sample_paths)}\")\n",
    "print(f\"len test_sample_paths  : {len(test_sample_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBG images and their corresponding edge images are concatenated, this is why we should split every sample data into half. One corresponding to the image and the other to the edges.\n",
    "This function read_image does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    width = tf.shape(image)[1]\n",
    "    width_half = width//2\n",
    "\n",
    "    input_image = image[:,:width_half,:]\n",
    "    target_image = image[:,width_half:,:]\n",
    "\n",
    "    input_image = tf.cast(input_image, dtype=tf.float32) \n",
    "    target_image = tf.cast(target_image, dtype=tf.float32)\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is an important step in data preprocessing, as it helps the deep learning model avoid the covariate shift, and speeds its learning.\n",
    "Every image is normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, target_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    target_image = (target_image / 127.5) - 1\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two previous two functions are combined in the preprocess_fn, which refer to the preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(image_path):\n",
    "    input_image, target_image = read_image(image_path)\n",
    "    input_image, target_image = normalize(input_image, target_image)\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a random sample is chosen from the dataset and ploted using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = random.choice(training_sample_paths)\n",
    "input_image, target_image = preprocess_fn(sample_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(8,8))\n",
    "axs[0].imshow(input_image)\n",
    "axs[1].imshow(target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can't load all our data into memory at once due to the large size of the dataset. We use the TensorFlow dataset, which plays the role of a generator that reads data from disk and preprocesses it on the fly. The advantage of using such a generator is that it optimizes the use of both CPU and GPU, that is by loading data using the CPU while the GPU is training the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(training_sample_paths)\n",
    "train_dataset = train_dataset.map(preprocess_fn, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(100)\n",
    "train_dataset = train_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for sample in train_dataset:\n",
    "    num += 1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is ready. The next step is to define the genrator and the discriminator networks that will be trained on the dataset on an adverserial way.\n",
    "Let's first begin by the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is a [U-Net](https://arxiv.org/pdf/1505.04597.pdf) like architecture. \n",
    "This architecture consists of an encoder that encodes the input image into a representation in a latent space. The second component is the decoder that maps the learned representation of the image into the desired output. \n",
    "This network was first designed for semantic segmentation. It gets an image as the input and learns the segmentation mask corresponding to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building block of the U-Net is convolution, pooling, and upsampling layers. The convolution extracts features, the pooling helps increase the receptive field of the encoder and the upsampling doubles the size of its input at each dimension . \n",
    "Both the encoder and the decoder use convolutional layers followed by batch normalization and relu activation. In order to avoid writing explicitly the operation for every step, it is better to group these repeated operations in one block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition du modele\n",
    "class Block(tf.keras.layers.Layer):\n",
    "    def __init__(self,  out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(out_ch, (4, 4), 2,kernel_initializer='he_normal', padding='same')\n",
    "        self.BatchNormalization1 = tf.keras.layers.BatchNormalization()\n",
    "        self.leaky_relu1  = tf.keras.layers.LeakyReLU(alpha=0.01)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.BatchNormalization1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        return x\n",
    "        \n",
    "    def model(self, input_shape):\n",
    "        x = tf.keras.Input(input_shape)\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the block has the desired behavior, a method called \"model\" was included in the Block class. This helps print the summary of the Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = Block(30)\n",
    "print(block.model((256,256,1)).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder consists of a series of convolutions followed by batch normalization and relu activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(tf.keras.layers.Layer):    \n",
    "    def __init__(self, chs=(32,64, 128, 256, 512)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = [Block(ch) for ch in chs]\n",
    "    \n",
    "    def call(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "        return ftrs\n",
    "        \n",
    "    def model(self, input_shape):\n",
    "        x = tf.keras.Input(input_shape)\n",
    "        return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder take an array of dimension (256,256,3) and outputs and array of dimension (1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(chs=(64, 128, 256, 512, 512, 512, 512, 512))\n",
    "print(encoder.model((256,256,1)).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every step, the decoder up-convolves its input then concatenates it with the corresponding output of the encoder, the operations in the Block are applied to the concatenated tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "# class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, chs=(64, 32, 16)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = [tf.keras.layers.Conv2DTranspose(ch, (4, 4), strides=(2, 2), padding='same') for ch in chs[1:]]\n",
    "        self.dec_blocks = [Block(chs[i]) for i in range(1, len(chs))]\n",
    "        \n",
    "    def call(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            x        = tf.keras.layers.concatenate([x, encoder_features[i]])\n",
    "        return x\n",
    "        \n",
    "    # def model(self, input_shape):\n",
    "    #     x = tf.keras.Input(input_shape)\n",
    "    #     return tf.keras.Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the generator is composed of the encoder followed by the decoder, and lastly another up-convolution layer in order for the output to have the same dimension as that of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, enc_chs=(64, 128, 256, 512, 512, 512, 512, 512), dec_chs=(512,512,512,512,512, 256, 128, 64),num_channels=3):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.conv = tf.keras.layers.Conv2DTranspose(num_channels, (4, 4), strides=(2, 2), padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.conv(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "generator.build(input_shape=(1,256,256,1))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator is equivalent to the encoder part of the Generator. It takes as input both the RBG image and the edges image concatenated together in the channels dimension. Then it outputs a probability map of dimension (30,30). The probability map indicates whether the edges image (drawing) corresponds to the RGB image or not. If the drawing corresponds to the RBG image (real data), the map should ideally be all ones, otherwise it should be all filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, enc_chs=[64,128,256]):\n",
    "        super().__init__()\n",
    "        self.initializer = tf.random_normal_initializer(0.0, 0.02)\n",
    "        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=self.initializer, use_bias=False)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=self.initializer, activation='sigmoid')\n",
    "        \n",
    "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.leaky_relu = tf.keras.layers.LeakyReLU()\n",
    "        self.enc_blocks = [Block(ch) for ch in enc_chs]\n",
    "\n",
    "    def call(self, x1, x2):\n",
    "        x = tf.keras.layers.concatenate([x1, x2])\n",
    "        for enc_block in self.enc_blocks:\n",
    "            x = enc_block(x)\n",
    "        \n",
    "        x = self.zero_pad1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.zero_pad2(x)\n",
    "        output = self.conv2(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def model(self, input_shape=[256,256,3]):\n",
    "        x1 = tf.keras.Input(shape=input_shape)\n",
    "        x2 = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "        return tf.keras.Model(inputs=[x1,x2], outputs=self.call(x1,x2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(enc_chs=[64,128,256])\n",
    "print(discriminator.model().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train both the generator and the discriminator a loss function should be defined. \n",
    "First of all, let's remember that the generator is given an edge image in its input and is required to output an RGB image of the edge image.\n",
    "The generator's loss function is a composed of two sub-functions:\n",
    "- The binary cross entropy: in order to push the generator in the direction of generating images that the discriminator will label as real (even though they aren't).\n",
    "- The Mean absolute error (between the original RBG image and the generated image): in order to measure the difference between the generated image, and the real one.\n",
    "\n",
    "The total loss is a weighted sum of both the binary cross entropy and the L1 loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target, real_labels):\n",
    "    Lambda = 100\n",
    "\n",
    "    loss1 = tf.keras.losses.BinaryCrossentropy()\n",
    "    loss2 = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    bce_loss = loss1(real_labels, disc_generated_output)  \n",
    "    l1_loss = loss2(target, gen_output)\n",
    "\n",
    "    total_gen_loss = bce_loss + Lambda*l1_loss\n",
    "\n",
    "    return total_gen_loss, bce_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Discriminator is given at its input a pair containing the drawing and its corresponding image. The discriminator is then required to classify the pair as being true or fake. Ideally, the true pairs should output a (30x30) probability map filled with ones,, and the fake pairs should output a map filled with zeros.\n",
    "The discriminator's loss is a binary cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output, real_labels, fake_labels):\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    bce_loss_real = loss(real_labels, disc_real_output)\n",
    "    bce_loss_fake = loss(fake_labels, disc_generated_output)\n",
    "    total_loss = bce_loss_real + bce_loss_fake\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the loss function is defined, an optimization strategy should be defined. The Adam optimizer was defined for both the Generator and the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "generator_optimizer = tf.keras.optimizers.Adam((2e-4), beta_1=0.5, beta_2=0.999)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam((2e-4), beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready for training the model. The training strategy is the following:\n",
    "- First , the generator is given a edge image (input_image) and generated an RGB image (gen_output). \n",
    "- The discriminator is given the edge image (input_image) and the true RGB image (target), and outputs a probability map (disc_real_output) corresponding to the likelihood that the pair is real.\n",
    "- The discriminator is given the edge image (input_image) and the generated RGB image(gen_output), and outputs a map (disc_generated_output) corresponding to the likelihood that the pair is real.\n",
    "- generate the ground truth probability map for the real pair, which is a map full of ones.\n",
    "- generate the ground truth probability map for the fake pair, which is a map full of zeros.\n",
    "- compute the generator's loss \n",
    "- compute the discriminator's loss\n",
    "- compute the gradient of the dicriminator's loss with respect to the discriminator's trainable parameters.\n",
    "- compute the gradient of the generator's loss with respect to the generator's trainable parameters.\n",
    "- apply the gradient to the discriminator's trainable parameters using the defined optimizer.\n",
    "- apply the gradient to the generator's trainable parameters using the defined optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    input_image, target = inputs\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator(input_image, target)\n",
    "\n",
    "        disc_generated_output = discriminator(input_image, gen_output, training=True)\n",
    "        \n",
    "        real_targets = tf.ones_like(disc_real_output)\n",
    "        fake_targets = tf.zeros_like(disc_real_output)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, l1_loss = generator_loss(disc_generated_output, gen_output, target, real_targets)\n",
    "\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output, real_targets, fake_targets)\n",
    "\n",
    "        gen_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "\n",
    "        return gen_gan_loss, l1_loss, disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training consists of iterating over all batches of the dataset for a determined number of epochs, and applying the train step function defined above.\n",
    "The losses are printed at the end of every epoch in order to follow the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        num_batches = 0\n",
    "        gan_loss, l1_loss, disc_loss = 0, 0, 0\n",
    "        for dist_inputs in train_dataset:\n",
    "            num_batches +=1\n",
    "            gan_l, l1_l, disc_l = train_step(dist_inputs)\n",
    "            gan_loss += gan_l\n",
    "            l1_loss += l1_l\n",
    "            disc_loss += disc_l\n",
    "\n",
    "        gan_loss = gan_loss/num_batches\n",
    "        l1_loss = l1_loss/num_batches\n",
    "        disc_loss = disc_loss/num_batches\n",
    "\n",
    "        print(f\"Epoch: {epoch}: D_Loss: {disc_loss}: G_Loss: {gan_loss}: l1_loss: {l1_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save_weights('gen'+str(100)+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_sample_paths)\n",
    "test_dataset = test_dataset.map(preprocess_fn, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(100)\n",
    "test_dataset = test_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "generator.build((1,256,256,3))\n",
    "generator.load_weights(\"gen100.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in test_dataset.take(1):\n",
    "    preds = generator(img, training=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(8,8))\n",
    "axs[0].imshow(img[0,:,:,:])\n",
    "axs[1].imshow(preds[0,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sources\n",
    "- https://learnopencv.com/paired-image-to-image-translation-pix2pix/\n",
    "- https://www.kaggle.com/datasets/balraj98/edges2shoes-dataset\n",
    "- https://amaarora.github.io/2020/09/13/unet.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c53109e521fc789122ce000cac189121e9080f7196eef2392df03f2361de8356"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ai': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
